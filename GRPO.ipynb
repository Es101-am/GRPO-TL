{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "fa038375f5144e359c862c2c6e80229b",
      "87cf6b528f834c23a93f122a04ddd337",
      "1b7a4a84d9374572b6fcb94f1850fe5f",
      "791bfdf26bd547c9ae70ff7df6a59f0c",
      "6dcdce9cd6734e3884dcd27ab5a0dcc5",
      "b6c98044f7074a2790d019fbd877b321",
      "7b6bc53129cb4f1c826c9d899db5bc62",
      "43e101c44bf44166bc3fe3f465a26f41",
      "910ae81d25a745439064fbae11da946d",
      "37b78cc8d487483ba047e855ac7672c1",
      "4a9bbad7105a4e59931884c0ce6632ac",
      "30d5036cd2fc46519e624245fd823126",
      "a7d8fec6000d4108a681afb6534089e7",
      "1e7fc1b5cbb340d9880c75da48148a17",
      "3b64453565aa4b798ee0ce8c045eec8d",
      "9a556b75c85b46d88c95abb47da3b7bc",
      "d8378a136aab44459f86fa7f80f3aa5b",
      "bfcc718c06de4054b1023dbeb39b90d6",
      "56bb98db4b8f45c88a2d90f1724dd620",
      "4715993af6414558b09173e92cf53ecd",
      "93f84ae10220484394ab7a30e3fc0326",
      "d09ba286e4aa4d8ba213f09f41d87ad0",
      "dc999eb674be45d7899a6afc3f644255",
      "9ac0ac2a79b74adaacd11d7a7a265eb5",
      "0d655f2e9cc7402084f7ff8271a594c8",
      "6a34ecb9560641c790c3150f043faaf6",
      "530a41ae8e48470a841500a442624e72",
      "ce9548d0c34c4ad6939d00a3c1ca3552",
      "d2e406cb04b346f5afd30f60e23b27c8",
      "fb1e452cd74d467c9e19fe2dc18b33f0",
      "8c67050670be41d7be0e9dd3db5c8650",
      "153b3a0ac2184adeac5c80c3853f45ad",
      "e8ab870f4309472d92040f19779dfd9f",
      "0f097cb74d7f421ea2f6452f051a6a25",
      "7dd572f7c9d84b0da77e948ecbf1d371",
      "371fdf0fc83c446fbc431edeb40fa3e1",
      "6c31ed5dee5a44ca8bc3d92631d2e53c",
      "170095f0a218497a9e34d7d16c99420c",
      "7647cd518c4d4bff8ab43a5f425c48ec",
      "f90b320a17d34e07bdc4e4a98d203b11",
      "94b2af4fd5ab413f862a21fe7511fa68",
      "dabf6bfbe85747b192a892fe3a31338c",
      "6e13d00a7fa148d8afe6e0a6782540a8",
      "ed042295928f4119889789c0b0dae69e",
      "bbcbc66d13bf44aea8f925497bbdad18",
      "dd4269ae7044434d84516d987e51e666",
      "a0ded1a9e5784e4a9ce7d13e692614ee",
      "e1f436a1cb334bcb8c91fe4c62bfa33b",
      "5e5f562ab0894bdc81b2d0c5a4d51630",
      "3a16bf84c7c84ed580db21cb9fe3dfa8",
      "61bce330cb3f45f295988204efb7d0bc",
      "7c5fdb848bb44f22be41fdf35fe5c145",
      "40af9d7d6b1347688ef5cc78ae56b7de",
      "f67e1c103ea44bfcb8339b490824e7d9",
      "4f5a4a78800049c1b7625f50c831e902",
      "17feb56629554ecead3942386bca7f9f",
      "dce776e7aca247ac8328bb255b3abd32",
      "bb09b9bbd106466c9cb66cccb8a1db4e",
      "3d6eb8687b574a0cbebd0fbdefdcc39a",
      "101a16dc093a44b7ae6594b5810c8950",
      "e1ded86991de49d5b9ab1954863a0b5a",
      "16850981284c448993b9924fde411fca",
      "abef0a1d165940989972d786372c39cf",
      "46b24f397b1e4f8b89809c7279717b3c",
      "04054ee00b3d4cdaae170edb76c3d38b",
      "fd2cb04543ec466e84c865c0fa5e796b",
      "d596ea086c0d42ee8e2be8465103abe1",
      "bb2ab64ddaab49b0bbc06b00f927cbd5",
      "b521f3b247524b169323810832365297",
      "c4bd80b5c47d4263b60fef900a2bca56",
      "9b9cfe42d8224817bdf4de4dddb0b73c",
      "edef79c42365457da3c08dff7a285a94",
      "1ad97c85d0e94cbab4e59a4dabdd4d1b",
      "679c3643596948549a8e555ed8e7c1c9",
      "f7d050b3b9ab4fdab3d41993865e583b",
      "d8010731c267494290ac206afb563fdc",
      "0704141b88814e0e88e4cf7cbf0d4d16"
     ]
    },
    "executionInfo": {
     "elapsed": 27288,
     "status": "ok",
     "timestamp": 1748188350976,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "rwbyhQ_px1Li",
    "outputId": "bb335e40-f8f8-4bfd-8e08-8835e263fd0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa038375f5144e359c862c2c6e80229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d5036cd2fc46519e624245fd823126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc999eb674be45d7899a6afc3f644255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f097cb74d7f421ea2f6452f051a6a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcbc66d13bf44aea8f925497bbdad18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17feb56629554ecead3942386bca7f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d596ea086c0d42ee8e2be8465103abe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # âœ… TinyLlama model (1.1B)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9a1de885d9d1414c8fb92d745d1a986c",
      "49e7963ad6fb4f90917a741445702003",
      "4e6971f207dc41538404aa7a181d22cb",
      "3e2759b164614430b964485d3e90054e",
      "cc830c63bc9849e293eb50694c33bfd3",
      "0c80cc84d74e41088ab5d20a6ed584bc",
      "61dea1a1bc7b4ae39b2f860a35e484c2",
      "e5f1c4ddee2946958405d20afe32d71c",
      "10ea38693a81448baabe20bbe9420c39",
      "4c29d64c82dc42dbaeb75766a2c99465",
      "5356eb848fd34f74b2c950254aa86a79",
      "bb4ccc03349d43478e8b6ed16ca9f475",
      "63c9f985140140bf92b78daa83cfeb0c",
      "a71ddacd4d7b43aab45ea0bbe45aab0b",
      "bd09aa4605284381bc04d4f6531af923",
      "0685011ce0eb4970b42ba1b0d803343a",
      "7df24374cfd2440dad25ece42f68c241",
      "1bc9211e0124428a9bd3d4aa321f28e6",
      "812664a12a5b4f648cdeb11dafa8189b",
      "514b735be69e4d76acae78f63ac5c0d6"
     ]
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1748188040133,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "lc7hfpcUIIEQ",
    "outputId": "0690cada-0a8e-434d-b606-14fb8ccb5c7f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1de885d9d1414c8fb92d745d1a986c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()  # Will prompt for your token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "a30ab7afdced454d97b51064400768eb",
      "7d3dc8824d084e8d96693112449ea2f7",
      "b5c1ffb8375f489e986d40d70a86a93e",
      "dcde93bf97be4ffb82ca19931330f50a",
      "4d7fbf66a7784be4b6d22e883ccf9b25",
      "58f9c7bb88eb435d9a72dbfb7beeea0f",
      "731784418b9f4659ba4f941e76821c34",
      "29d26a48a5fb4f159270de75321b60bc",
      "6f4e7af61ec34dd999c5dd26e543bb1d",
      "b197a5484df44c559b25af5dd6e62420",
      "88c6cd7b0d2f4025a2a254518572c4a3",
      "49ad0e2c552d4ea2b6811e5136b30ec5",
      "421bcf3e6c014556bb9b5bcc93daf9ca",
      "426ad02f64e04c93bcc0dd10a5d34145",
      "8e466f3b50c24894b3e48eceb3893112",
      "80c337f5e9744ec686c63d3ab941b207",
      "3f9402c190f64a5faec3b15cd8927dcf",
      "33430749b7af437894033fb24ce4441f",
      "1664303d80b94d47803b5713536deeeb",
      "0e7a1c2f18f24cc8887dc12e6ae7ddc7",
      "db08a87e81cb48f0aac1a9e5e9dfaecf",
      "52de8730122f4827a983d59e2520a5a4",
      "dee8b18fba5247d39c6a6f22aef1628f",
      "e59b6acf5a914852b68b4df32c3a8801",
      "5cd25f93f01b49d796379a505f946736",
      "149360bc0629465582ff01774b3ec3bf",
      "1f767e7649e74559836893e4f1c4f3eb",
      "4b9a4003044e4fe79d72f97b7a1695f3",
      "d360c83ff9994f67abfc712a144f399e",
      "a422084a6cd34beb8a3334228abfa893",
      "9fa80bcee7f444e0881aeae13ede2a3f",
      "a0ab9d16435b454db40ea02abfd18866",
      "9e2cca4b510248b9a39360b07d7535fe",
      "189f7c354a9c4352a757b2a03867931a",
      "91f723dd47764412baca87104d1e5816",
      "8222588f69e44787b343e3ce2b4cf1c4",
      "b6b0c8ce2b18464f827dafd3205400a2",
      "5f3e231f7fe84930a482ef4716cc93c1",
      "a949a2b1d8b44d7dae8d13be1db7aa54",
      "6c83dc5238f44c4ea4c58d10bc927a1c",
      "b821dcf082fd4d9092708c4dce04a23b",
      "f0bbef332bd24d58827e2d5f132b01e9",
      "26173410cb6047ad8904ffdbaaf77bf7",
      "4d5d80248ef440259a9ec40f8f8d1a78",
      "f71c784e803b4d2e905f82a8121e519b",
      "932892ad640f4a97a051ded923231bd1",
      "918e1be5782241d3aa93f4dfe9cb3c74",
      "e128b3b33a464a369a939b0ee5e87485",
      "7bc39da3dff846a8a62a3c89606c280f",
      "a3f49a61f07b4f558f1997e040b751de",
      "3adf2b721dea4e7597a6e3257cfdf27a",
      "3bc5855d7aaf4a35b164a40dbd09a0ae",
      "69e182452ddb422090f11009dd15cc58",
      "9d7baab4754a454288f7b7898d280fc9",
      "3ee271d9cb3a43d4a88b3055ab7c999e"
     ]
    },
    "executionInfo": {
     "elapsed": 6924,
     "status": "ok",
     "timestamp": 1748188431152,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "M4BONsil01-D",
    "outputId": "07845474-4c83-479e-b30f-72f56f9df556"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30ab7afdced454d97b51064400768eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ad0e2c552d4ea2b6811e5136b30ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee8b18fba5247d39c6a6f22aef1628f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189f7c354a9c4352a757b2a03867931a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71c784e803b4d2e905f82a8121e519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset (default config is 'main')\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")  # or \"default\" if you prefer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1748188468529,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "VoHYqvAzJvkU"
   },
   "outputs": [],
   "source": [
    "def generate_gsm8k_prompt(example, tokenizer, split=\"train\"):\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    r1_prefix = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. You first think about the reasoning process step by step and then provide the user with an answer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{question} Please show your reasoning inside <think> </think> tags and your final answer inside <answer> </answer> tags.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True),\n",
    "        \"target\": example[\"answer\"],\n",
    "        \"question\": question,\n",
    "        \"split\": split\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1748188855296,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "fPt-R3fiKMfl"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "train_subset = dataset[\"train\"].select(range(200))\n",
    "\n",
    "# Map the prompt generator to the train split\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    lambda x: generate_gsm8k_prompt(x, tokenizer),\n",
    "    desc=\"Formatting train prompts\"\n",
    ")\n",
    "\n",
    "# Optional: Concatenate with test split\n",
    "# test_dataset = dataset[\"test\"].map(lambda x: generate_gsm8k_prompt(x, tokenizer), desc=\"Formatting test prompts\")\n",
    "# full_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "full_dataset = train_dataset\n",
    "\n",
    "# Convert to pandas for saving\n",
    "df = full_dataset.to_pandas()\n",
    "\n",
    "# Save to JSONL file\n",
    "df.to_json(\"gsm8k_formatted.json\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1748188857231,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "lcKQS6rjKU2y",
    "outputId": "ea733995-30fc-46dc-a228-5095a03594e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'prompt': '<|system|>\\nYou are a helpful assistant. You first think about the reasoning process step by step and then provide the user with an answer.</s>\\n<|user|>\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Please show your reasoning inside <think> </think> tags and your final answer inside <answer> </answer> tags.</s>\\n<|assistant|>\\nLet me solve this step by step.\\n<think>', 'target': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first processed example\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1748188871918,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "LhGOjcSmLNSX",
    "outputId": "8a543345-3d26-499b-fa24-f70d31099c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¨ Prompt:\n",
      " <|system|>\n",
      "You are a helpful assistant. You first think about the reasoning process step by step and then provide the user with an answer.</s>\n",
      "<|user|>\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Please show your reasoning inside <think> </think> tags and your final answer inside <answer> </answer> tags.</s>\n",
      "<|assistant|>\n",
      "Let me solve this step by step.\n",
      "<think>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the first example\n",
    "with open(\"gsm8k_formatted.json\", \"r\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "\n",
    "prompt = first_line[\"prompt\"]\n",
    "print(\"ðŸ“¨ Prompt:\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14194,
     "status": "ok",
     "timestamp": 1748188900689,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "khRmgzHkLU7B",
    "outputId": "0989668d-f245-4435-fa20-7377cfd0f9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Model Response:\n",
      " \n",
      "- Natalia sold clips to 48 of her friends in April.\n",
      "- Then she sold half as many clips in May.\n",
      "\n",
      "- How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "- Let's say she sold 36 clips in April and 24 in May.\n",
      "\n",
      "- So, she sold a total of 60 clips in both April and May.\n",
      "\n",
      "- Now, let's calculate the number of clips she sold in April.\n",
      "\n",
      "- Let's say she sold 18 clips in April.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode only the completion\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nðŸ¤– Model Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1748188998213,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "SdRMlM37JphQ"
   },
   "outputs": [],
   "source": [
    "def format_reward_func(prompt: str, completion: str, example: dict[str, str]) -> float:\n",
    "    import re\n",
    "\n",
    "    try:\n",
    "        reward = 0.0\n",
    "        completion_lower = completion.lower()\n",
    "\n",
    "        # âœ… Reasoning present?\n",
    "        reasoning_keywords = [\n",
    "            \"step\", \"first\", \"then\", \"calculate\", \"next\", \"approach\", \"let's\", \"we get\"\n",
    "        ]\n",
    "        if any(kw in completion_lower for kw in reasoning_keywords):\n",
    "            reward += 2.0\n",
    "\n",
    "        # âœ… Uses <think> and <answer> tags properly\n",
    "        if \"<think>\" in completion and \"</think>\" in completion:\n",
    "            reward += 1.0\n",
    "        if \"<answer>\" in completion and \"</answer>\" in completion:\n",
    "            reward += 1.0\n",
    "\n",
    "        # âœ… Correct final answer?\n",
    "        correct_answer_match = re.search(r\"answer is (\\d+)\", example[\"answer\"].lower())\n",
    "        model_answer_match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", completion)\n",
    "\n",
    "        if correct_answer_match and model_answer_match:\n",
    "            correct = correct_answer_match.group(1).strip()\n",
    "            model = model_answer_match.group(1).strip()\n",
    "\n",
    "            # Allow numeric match (you can make this more flexible with eval)\n",
    "            if correct == model:\n",
    "                reward += 5.0\n",
    "\n",
    "        return reward\n",
    "\n",
    "    except Exception:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1748189252693,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "j9nb5xsYL5AS",
    "outputId": "7c1112bb-4359-4643-f50f-5f35369dc581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# âœ… Apply LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# âœ… Load your pandas DataFrame (already prepared)\n",
    "# hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# âœ… GRPOConfig\n",
    "\n",
    "\n",
    "# grpo_config = GRPOConfig(\n",
    "#     output_dir=\"./qwen1.5-countdown-grpo\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=1,\n",
    "#     learning_rate=5e-6,\n",
    "#     logging_steps=2,\n",
    "#     save_steps=0,\n",
    "#     warmup_steps=0,\n",
    "#     bf16=False,                              # âœ… Set False if Colab doesnâ€™t support BF16\n",
    "#     fp16=True,\n",
    "#     remove_unused_columns=False,\n",
    "#     max_prompt_length=256,\n",
    "#     max_completion_length=64,\n",
    "#     num_generations=2\n",
    "# )\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"./qwen1.5-countdown-grpo\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=4,                    # âœ… More frequent logging\n",
    "    report_to=\"none\",                   # âœ… Disable W&B logging for now\n",
    "    save_steps=10,                      # Optional\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=64,\n",
    "    num_generations=2                  # âœ… Reduce to 1 for faster runs\n",
    ")\n",
    "\n",
    "\n",
    "def combined_reward_fn_factory(dataset):\n",
    "    def reward_fn(prompts, completions, **kwargs):\n",
    "        rewards = []\n",
    "        for i, (prompt, completion) in enumerate(zip(prompts, completions)):\n",
    "            try:\n",
    "                example = dataset[i]\n",
    "\n",
    "                format_reward = format_reward_func(prompt, completion, example)\n",
    "                # equation_reward = equation_reward_func(prompt, completion, example)\n",
    "\n",
    "                # total_reward = format_reward + equation_reward\n",
    "                total_reward = format_reward\n",
    "                rewards.append(total_reward)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Reward function failed at index {i}: {e}\")\n",
    "                rewards.append(0.0)\n",
    "        # print(rewards)\n",
    "        return rewards\n",
    "\n",
    "    return reward_fn\n",
    "\n",
    "# def combined_reward_fn_factory(dataset):\n",
    "#     def reward_fn(prompts, completions, **kwargs):\n",
    "#         # âœ… Skip format_reward_func and equation_reward_func for now\n",
    "#         print(f\"[DEBUG] Scoring {len(prompts)} completions...\")  # Optional logging\n",
    "#         return [1.0 for _ in prompts]  # Return fixed reward\n",
    "#     return reward_fn\n",
    "\n",
    "reward_fn = combined_reward_fn_factory(train_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3146445,
     "status": "ok",
     "timestamp": 1748192404272,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "y9cf7453MVdT",
    "outputId": "1d473347-b3c0-4cd2-e5a9-71612977b41a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='467' max='467' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [467/467 52:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>-0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>-0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=467, training_loss=0.0010722211618933676, metrics={'train_runtime': 3145.4944, 'train_samples_per_second': 2.376, 'train_steps_per_second': 0.148, 'total_flos': 0.0, 'train_loss': 0.0010722211618933676})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… Initialize GRPOTrainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=grpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    reward_funcs=reward_fn,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7651,
     "status": "ok",
     "timestamp": 1748192420084,
     "user": {
      "displayName": "Esmaeil Amiri",
      "userId": "14685918732015466754"
     },
     "user_tz": -210
    },
    "id": "WcquoU-7Yze-",
    "outputId": "a58c714d-2771-46d8-920f-37ad6e8dbb53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Model Response:\n",
      " \n",
      "First, let's think about the reasoning process step by step.\n",
      "\n",
      "Step 1: Calculate the number of clips sold by Natalia in April and May.\n",
      "Step 2: Calculate the total number of clips sold by Natalia throughout the month of April and May.\n",
      "Step 3: Multiply the total number of clips sold by April and May by the percentage of sales in April and May.\n",
      "Step 4: Divide the result by 48 to get the average number of clips sold per month.\n",
      "Step 5: Add 48 clips to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode only the completion\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nðŸ¤– Model Response:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNxxNqcIB4NXvSluIuI3k3T",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
